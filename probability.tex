

\paragraph{Sample Space and Outcome}
We perform random experiments and the sample space is the set of possible
outcomes.

For example, consider rolling a die. The set of possible outcomes are:
\begin{equation*} \begin{split}
	S = \{1,2,3,4,5,6\}
\end{split} \end{equation*}

\paragraph{Event} An event is a subset of the sample space. An example event is
rolling a die and getting an even odd outcome:
\begin{equation*} \begin{split}
	E = \{1,3,5\}
\end{split} \end{equation*}

\paragraph{Disjunction of Events} The event $E$ occurs if $E_1$ or $E_2$ occur.
Another way to imagine this is the union of events: $E = E_1 \cup E_2$.

\paragraph{Conjunction of Events} The event $E$ occurs if $E_1$ and $E_2$ occur.
Another way to imagine this is the intersection of events: $E = E_1 \cap E_2$.
Some alternative ways of writing this are:
\begin{equation*} \begin{split}
	P(E) &= P(E_1 \cap E_2) \\
	P(E) &= P(E_1 \wedge E_2) \\
	P(E) &= P(E_1\, ,  E_2) \\
	P(E) &= P(E_1\,  E_2) 
\end{split} \end{equation*}

\paragraph{Mutually Exclusive Events} Events $E_1$ and $E_2$ are mutually
exclusive if only one of them can occur in a single experiment. For example, the
event rolling an even number and the event rolling an odd number on a die are
mutually exclusive events:
\begin{equation*} \begin{split}
E_{even} \cap E_{odd} =  \{1,2,3,4,5,6\} \cap \{1,3,5\} = \varnothing
\end{split} \end{equation*}

\paragraph{Axioms of Probability} The are the rules we accept as truth without
proof. We build probability untop of these axioms.

\begin{enumerate}
	\item $0 \le P(E) \le 1$, for any event $E$. In the smallest case, the event
		cannot occur which is inidicated by a probability of 0. In the largest case,
		the event always occurs, which is indicated by the probability of 1.
	\item $P(S) = 1$, where S is the sample space. The sample space contains all
		possible outcomes for each experiment. It's reasonable to accept that an
		event from the sample space always occurs.
	\item For a potentially infinite set of mutually exclusive events $E_1$,
		$E_2$, ... 
		\begin{equation*} \begin{split}
			P( \cup_{i=1}^\infty E_i ) = \sum_{i=1}^\infty P(E_i)
		\end{split} \end{equation*}
		It makes senses that events that do not share outcomes for a single event,
		can have their probabilities added to arrive at the probability of combining
		the outcomes from the events.
\end{enumerate}

\paragraph{Properties} From the above axioms, we get the following useful
properties (TODO proof): %TODO proof
\begin{enumerate}
	\item For any event $E$, let $\overline{E}$ be the complement of $E$. More
		concretely, $\overline{E} = S - E$, where S is the sample space. Then $E$
		and $\overline{E}$ are mutually exclusive.
	\item $P(\varnothing) = 0$ You can never get none of the outcomes of the
		sample space.
	\item If $E_1$ and $E_2$ are mutually exclusive events then
		\begin{equation*} \begin{split}
			P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1\, , E_2)
		\end{split} \end{equation*}
\end{enumerate}

%TODO probability as a limit? should this be included
% ex/ network measurement
% 450,000 out of 1,000,000 packets are UDP
% P(UDP) =   400,000
%          ----------
%          1,000,000

%TODO subjective probability
% no data to back this up
% using an expert's opinion

\paragraph{Conditional Probability} We use conditional probability to model the
probability given knowing some circumstance has happened.

Given two event $E$ and $F$, the conditional probability, the probability of $F$
given $E$ has occured, is defined as ($P(E) \ne 0$):
\begin{equation*} \begin{split}
	P(F|E) = \frac{P(E\, ,F}{P(E)}
\end{split} \end{equation*}

An example is what is the probability of rolling a 3, given that we rolled an
odd number. Let $F = \{3\}$ and $E = \{1,3,5\}$:
\begin{equation*} \begin{split}
	P(F|E) = \frac{P(E\, , F)}{P(E)} = \frac{\frac{1}{6}}{\frac{1}{3}} =
	\frac{1}{2}
\end{split} \end{equation*}

\paragraph{Joint Probability} In queing theory, we often have to use multiple
sample sapce. The theory in this book so far has covered only a single
probability space.

Suppose we have two sample spaces $S_1$ and $S_2$. The outcomes of the joint
probability space are the tuples that result from the cross product of the two
sample spaces:
\begin{equation*} \begin{split}
	S_{joint} = S_1 \times S_2
\end{split} \end{equation*}

For example, consider rolling a die and a coin
\begin{equation*} \begin{split}
	S_{die} =  & \{1,2,3,4,5,6\} \\
	S_{coin} = & \{H,T\} \\
	S =        & S_{die} \times S_{coin} \\
	S =        & \{(1,H),(2,H),(3,H),(4,H),(5,H),(6,H), \\
	           & (1,T),(2,T),(3,T),(4,T),(5,T),(6,T)\}
\end{split} \end{equation*}

\paragraph{Marginal Probability} Given the joint probabilties, we might want to
compute the probabilties of only one of the sample spaces. 

For example, suppose that we know the joint probability of the number of jobs at
server 1 and server two and we want to compute the probability of the number of
jobs at server 1 only. We can apply Marginal probability to determine that.

\begin{equation*} \begin{split}
\end{split} \end{equation*}



