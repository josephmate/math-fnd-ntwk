
\paragraph{Sample Space}: set of all possible outcomes
\paragraph{Event}: a subset of the space space

\paragraph{Disjunction of Events}:  $E = E_1 \cup E_2$
\paragraph{Conjunction of Events}:  $E = E_1 \cap E_2$ also written as $E_1 E_2$

\paragraph{Mutually Exclusive}: $E_1$ and $E_2$ are mutually exclusive $ \iff E_1 \cap E_2 = \varnothing $

\paragraph{Axioms of Probability}
\begin{align*}
	0 \le P(event) & \le 1 \\
	P(SampleSpace) &= 1 \\
	P(\cup_{i=1}^\infty E_i ) &= \sum_{i=1}^\infty P(E_i) \mbox{ where all } E_i \mbox{ are mutually exclusive}
\end{align*}

\paragraph{Corollaries from Axioms}
Let $E^c$ be the compliment of E, then E and $E^c$ are mutually exclusive \\
$P(\varnothing) = 0$ \\
If $E_1$ and $E_2$ are not mutually exclusive, then $P(E_1 \cup E_2) = P(E_1) +
P(E_2) - P(E_1 E_2)$.

\paragraph{Conditional Probability} $P(F|E) = \frac{P(EF)}{P(E)}$
\paragraph{Joint Probability} You have n samples space to draw from. The Joint
Probability is the cross product of those spaces.
IE:
$S_1 \times S_2 \times ... \times S_n$
and joint probability given by $E_1 \in S_1 , E_2 \in S_2 , ... , E_n \in S_n$
and
$P(E_1 , E_2  , ... , E_n )$

\paragraph{Marginal Probability}: Sum over all other sample spaces:
$P(S_1) = \sum_{j_2=1}^{N^2}...\sum_{j_n=1}^{N^n} P(E_1 , E_2  , ... , E_n ) $

\paragraph{Independent Events} Events E and F are independent 
$\iff P(EF) = P(E)P(F)$ equivalently $P(F|E) = P(F)$

\paragraph{Total Probability} $P(F) = \sum_{i=1}^n P(F | E_i )P(E_i)$

\paragraph{Bayes Rule} $P(E|F)=\frac{P(F|E)P(E)}{P(F)}$
\paragraph{Generalized Bayes Rule}
$P(E_i|F)=\frac{P(F|E_i)P(E_i)}{\sum_{j} P(F|E_j)P(E_j)}$


\subsubsection{Discrete}
\paragraph{Random Variable} A variable that can take on any value from the
sample space, along with a probability distribution on which it takes those
values.
\paragraph{Probability Mass Function} $P(X=x_i)$ shortened sometimes to $P(x_i)$
\paragraph{Mean} $E[X] = \sum_{i=1}^n x_i P(x_i)$
\paragraph{Variance} $var(X) = \sum_{i=1}^n (x_i - E[X])^2P(x_i)$
\paragraph{Cumulative Distribution Function}
\begin{align*}
	F(x)       &= \sum_{i \le x} P(i) \\
	F(-\infty) &= 0 \\
	F(\infty)  &= 1 \\
	F(b)-F(a)  &= P(a < X \le b) = \sum_{a<i \le b} P(i)
\end{align*}

\paragraph{Bernoulli} $P(1) = a$, $P(0) = 1-a$, $E[X] = a$, $var(X) = a(1-a)$
\paragraph{Binomial} n independant Bernoulli trials:
$P(i) = {n \choose i} a^i(1-a)^{n-j}$,
$E[X] = na$,
$var(X) = na(1-a)$
\paragraph{Poisson} $P(i) = \frac{e^{-\lambda} \lambda^i}{i!}$, 
$E[X] = \lambda$, 
$var(X) = \lambda$
\paragraph{Geometric} Prob that the $i^{th}$ Bernoulli trial is a success:
$P(i) = (1-a)^{i-1}a$,
$E[X] = \frac{1}{a}$,
$var(X) = \frac{1-a}{a^2}$,
has the memoryless property
\paragraph{Memoryless Property} $P(X=i+k|X>k) = P(X=i)$

\subsubsection{Continuous}
\paragraph{Probability Density Function} 
\begin{align*}
	P(x_1 \le X \le x_2) &= \int_{x_1}^{x_2} f(x) dx \\
	f(x) &\ge 0 \\
	\int_{-\infty}^{\infty} f(x) &= 1 \\
	P(X=x)              &=        0 \\
	P(x \le X \le x+dx) & \approx f(x)dx \mbox{ when x is small}
\end{align*}
\paragraph{Cumulative Distribution Function} 
$F(x) = P(X\le x) = \int_{-\infty}^{x} f(y) dy$
\paragraph{Expectation}
$E[X] = \int_{-\infty}^{\infty} x f(x) dx$
\paragraph{Variance}
$var(X) = \int_{-\infty}^{\infty} (x-E[X])^2 f(x) dx$

\paragraph{Uniform}
$f(x) = \frac{1}{b-a}$ if $a \le x \le b$, 0 otherwise, 
$E[X] = \frac{a+b}{2}$, 
$var(X) = \frac{(b-a)^2}{12}$, 
$F(x) = \frac{x-a}{b-a}$
\paragraph{Exponential}
$f(x) = \lambda e^{-\lambda x}$, 
$E[X] = \frac{1}{\lambda}$, 
$var(X) = \frac{1}{\lambda^2}$, 
$F(x) = 1 - e^{-\lambda x}$,
has memoryless property
\paragraph{Memoryless Property}
$P(Y \le x_0 + x | Y > x_0)$
\paragraph{Normal Distribution}
$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}$, 
$E[X] = \mu$, 
$var(X) = \sigma^2$
\paragraph{Power Law}
$f(x) = \frac{\alpha - 1}{x_m} \left( \frac{x_m}{x} \right)^\alpha$ if $x \ge
x_m$, 0 otherwise, 
$E[X] = \frac{\alpha-1}{\alpha-2} x_m$
\subsubsection{Joint Discrete Random Variables}
\paragraph{Joint Distribution} $P(X=i, Y=j) = P_{XY}(i,j)$ sometimes shortened to $ P(i,j)$
\paragraph{Marginal Distribution} $P_X(i) = \sum_{j} P(i,j)$
\paragraph{Condiitonal Probability} $P(j|i) = P(Y=j|X=i)$
\paragraph{Total Probability} $P_Y(j) = \sum_i P(j|i)P_X(i)$
\paragraph{Conditional Expecation} $E[Y|X=i] = \sum_j j P(j|i)$
\paragraph{Total Expecation} $E[Y] = \sum_i E[Y|X=i] P_X(i)$
\subsubsection{Joint Continuous Random Variables}
\paragraph{Joint Density Function}
$f_{XY}=(x,y) = f(x,y)$, 
$P(a \le X \le b, c \le Y \le d) = \int_a^b \int_c^d f(x,y) dy dx$
\paragraph{Marginal Density}
$f_X(x) = \int_{-\infty}^{\infty} f(x,y) dy$
\paragraph{Conditional Density}
$f_Y(Y|X=x) = \frac{f(x,y)}{f_X(x)}$
\paragraph{Total Probability}
$f_Y(Y) = \int_{-\infty}^{\infty} f_Y(Y|X=x) f_X(x) dx$
\paragraph{Conditional Expectation}
$E[Y|X=x] = \int_{-\infty}^{\infty} y f(y|X=x) dy$
\paragraph{Total Expectation}
$E[Y] = \int_{-\infty}^{\infty} E[Y|X=x] f_X(x) dx$
\subsubsection{Independant Random Variables}
\paragraph{Discrete} X,Y independant $\iff P(i,j) = P(i)P(j)$
\paragraph{Continuous} X,Y independant $\iff f(x,y) = f_X(x) f_Y(y)$
\subsubsection{Function of One Random Variable}
$Y= g(X)$
\paragraph{Discrete Expecation}
$E[Y] = \sum_i g(i)P(i)$
\paragraph{Continuous Expectation}
$E[Y] = \int_{-\infty}^{\infty} x g(x) f(x) dx$
\paragraph{Useful Relationships}
\begin{align*}
	E[aX+b] = aE[X] + b \\
	var(X+a) = var(X) \\
	var(aX) = a^2 var(X)
\end{align*}
\subsubsection{Function of Two Random Variables}
$Z =g(X,Y)$
\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}










