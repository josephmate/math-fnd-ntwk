
A family of random variables, indexed by time

\subsubsection{Classifications}

\paragraph{State Space} The set of possible values (states).
\paragraph{Discrete State Space} Example: the number of jobs in the system.
($S = {0,1,2,3...}$). We will only deal with the discrete case in this class. To
make notation easier the state is usually identified by the number.

\paragraph{Continuous State Space} Example: motion of a particle. We will not be
studying this in this class.

\paragraph{Time Parameter} There are two ways to observe times in Stochastic
processes.

\paragraph{Discrete Time Parameter} We consider the states at
$X_0$, $X_1$, ... $X_n$, and so on. For example, looking at the state of the
system at the $i^{th}$ hour.

%TODO fix diagram
%\input{./diagrams/stochastic-processes/discrete-time.tex}

\paragraph{Continuous Time Parameter}
The states are function of time $t$, $X(t)$.

\subsection{Discrete State Space and Time} There might be a dependency
between the previous time interval $X_i$'s and the states those time interval
can be in that need to be model in the current $X_n$. 
\begin{equation*} \begin{split}
	P(X_{n+1} = j | X_n = i_n\, , X_{n-1} = i_{n-1}\, , ... \, , X_1 = i_1\, ,X_0 = i_0)
\end{split} \end{equation*}
The number of dependency combinations is exponential because \\
$X_{n+1}$ depends on $X_n$ to $X_0$ \\
and $X_{n}$ depends on $X_{n-1}$ to $X_o$ \\
and so on. 

\paragraph{Markov Chain} As a result of the exponential size, we make a
simplifying assumption. We only use the latest information. $X_{n+1}$ only
depends on $X_n$. Now we are left with transition probabilities:
\begin{equation*} \begin{split}
	P(X_{n+1} | X_n) = P(X_{n+1} = j | X_n = i_n\, , X_{n-1} = i_{n-1}\, , ... \, , X_1 = i_1\, ,X_0 = i_0)
\end{split} \end{equation*}
Given $P(X_0 = i)$ for all $i$'s, we can compute any state. However, notice that
the formula depends on n, the discrete time that has pasted so far, which make
analysis difficult still. For example, in the 9am one hour interval the number
of jobs in a login system tends to be higher than at 2am.

\paragraph{Homogeneous Markov Chains} We now make the assumption that that the
transistion probabilities do not depend on time. For example, the transition
probabilies for the number of webcrawling robots requesting a webpage remain the
same despite the time. So we can write
\begin{equation*} \begin{split}
	P(X_{n+1} = j | X_n = i ) = P(X_{n} =j | X_{n-1} = i ), \forall n\,i\,j \ge 0
\end{split} \end{equation*}
Which is abbreviated to $P_{ij}$, the probability of going from state $i$ to state
$j$.


\paragraph{State Transistion Diagram} It's a diagram that enumerates all
possible state transistions, and annotates the edges with the probabilities of
going from one state to the next. Below is an example of a state transistion
diagram.
\input{./diagrams/stochastic-processes/state-diagram.tex}

\paragraph{Chapman-Kolmogorov Equation}  \label{chap-komo-eq-explain}
Let $P_{ij}^{(m)}$ be the m step transistion probability from state $i$ to $j$
defined as:
\begin{equation*} \begin{split}
	P_{ij}^{(m)} = P(X_{n+m} = j | X_n = i)
\end{split} \end{equation*}
To take m steps to go from (possibly visiting m in an intermediate state
multiple itmes), you can take m-1 to steps. At step m-1, you can arrive at any
state k. To go from each state k at the m-1 step to the m step, you just apply
the transisition probability.

We can sum the probabilities because the probabililites of going from state k in
the m-1 step to state j are mutually exclusive (they are mutually exclusive
because they are different states).

Lastly, we can mulitple the probability of going to state k in m-1 steps by the
probability of going to state j because the probabilities are independent.
\begin{equation*} \begin{split}
	P_{ij}^{(m)} = \sum_k p_{ik}^{(m-1)} p_{kj}
\end{split} \end{equation*}
Which is exactly the same as taking 1 step, and then m-1 steps.
\begin{equation*} \begin{split}
	P_{ij}^{(m)} = \sum_k p_{ik}p_{kj}^{(m-1)}
\end{split} \end{equation*}
For a derivation of this, see (\ref{chap-komo-eq-deriv}).


\paragraph{Irreducible Markov Chain} allows every state to be reached from every
other state for all pairs of states $i$ and $j$. More concretely:
\begin{equation*} \begin{split}
	\forall i \forall j \ne i : \exists m_{ij}  : P_{ij}^{(m_{ij})} > 0 
\end{split} \end{equation*}

\paragraph{Recurrent State}: State j is recurrent if after leaving state j then
you are guarenteed to eventually comeback.

Let $f_j^{(n)}$ be the probability that you first return to state j in n steps.

Notice that $f_j^{(0)} = 0$ because it's impossible to comeback without taking any
steps. Also notice that $f_j^{(1)} > 0$ is only possible is the transistion pointing
to itself $P_{jj} > 0$.

j is recurrent if and only if
\begin{equation*} \begin{split}
	f_j = \sum_{n=1}^\infty f_j^{(n)} = 1
\end{split} \end{equation*}

\paragraph{Recurrent Non-null} A state j is recurrent non-null if and only if we
get back to state j, but it does not take forever. More concretely: \\[0.5cm]
j is recurrent non-null if and only if j is recurrent ($f_j = 1$) and
\begin{equation*} \begin{split}
	M_j = \sum_{n=1}^\infty n f_j^{(n)} < \infty
\end{split} \end{equation*}
Where $M_j$ is the expected number of steps to come back.

\paragraph{Recurrent Null} j is recurrent null if and only if j is recurrent and
j is not recurrent non-null.


TODO EXAMPLE WITH DIAGRAM

TODO EXAMPLE WITH DIAGRAM

\paragraph{Periodicity} A state j is periodic if and only if the only way to
come back to state j is to take r, 2r, 3r, ... , cr, steps.  \\[0.5cm]

If a state j is not periodic, it's called aperiodic \\[0.5cm]

If state j as a self loop ($p_{jj} > 0$), then state j is aperiodic \\[0.5cm]

If the system is a irreducible Markov Chain, and contains a self loop, then all
states j are aperiodic.

\paragraph{State Probability} 
Let $X_n$ be the random variable for the state at interval n, then in a
homogeneous Markov Chain we have that:
\begin{equation*} \begin{split}
	\pi_j^{(n}) = P(X_n = j) \mbox{ - at step j }
\end{split} \end{equation*}
Let $p_{ij}$ be the transition probability for going from state $i$ to state $j$
(independant of time, so it's the same for all intervals ($X_n$), then
\begin{equation*} \begin{split}
	\pi_j^{(n+1)} = \sum_{i=0}^\infty p_{ij} \pi_i^{(n)}
	\mbox{ (by applying total probability)}
\end{split} \end{equation*}

Given initial conditions $\pi_j^{(0)} \forall j$, we can compute all
$\pi_j^{(i)}$ apply the above formula recursively.

\paragraph{Steady State Therom} (Equilibrium Probability Theorm) (I made up this name. It seems better
than 'Fundamental Theorm.)

If a homogeneous Markov Chain is irreducible and a periodic, then there exists a
limiting probability (equilibrium):
\begin{equation*} \begin{split}
	\pi_j = \lim_{n \to \infty} \pi_j^{(n)}
\end{split} \end{equation*}
which is independant of the initial conditions $\pi_j^{(0)}$. \\[0.5cm]

Moreover, if all states j are recurrent non-null, then $\pi_j$ is non zero and
can be uniquely etermined from the equations:
\begin{equation*} \begin{split}
	\pi_j = \sum_i p_{ij} \pi_i , \forall j
\end{split} \end{equation*}
\begin{equation*} \begin{split}
	\sum_j \pi_j = 1
\end{split} \end{equation*}

\subparagraph{Example}
\subparagraph*{}
\input{./diagrams/stochastic-processes/state-diagram.tex}

\subparagraph*{}
Let the initial state be (1). This is the 'initial condition'

\subparagraph*{}
\begin{tabular}{ r | r  | r  |  r  |  r  |  r  |  r   | r }
	  n & 0 & 1  & 2  & 3  & 4  & ...  & $\infty$ \\
	\hline
	$\pi_1^{(n)}$ & 1 & 0    & .25   & .187  & .203  &  & .2 \\
	$\pi_2^{(n)}$ & 0 & .25  & .062  & .359  & .254  &  & .28 \\
	$\pi_3^{(n)}$ & 0 & .75  & .688  & .454  & .543  &  & .52 \\
\end{tabular}

\subparagraph*{}
To get the steady state column, you have to solve the system of equations that
results from the above theorm. From the table we can see that the emperical
basis for the steady state theorm. As you compute more columns, it approaches the values
computed from the steady state equations.


\paragraph{Residence Time in a State} The residence time, is the amount of time
that the system spends in state j for m steps, given that we are already in
state j.
\begin{equation*} \begin{split}
	P( \mbox{in state j for m steps} | \mbox{already in state j}) \\
	= P( \mbox{in state j for m steps} \wedge \mbox{step m+1 is not j} | \mbox{already in state j})
\end{split} \end{equation*}
We have that the transistion probabilities are independent of time
(homogeneous), so we can multiple the individual probabilities. $p_{jj}$ is the
homogeneous transistion probability of going from state j to state j.
\begin{equation*} \begin{split}
	= p_{jj}^m(1-p_{jj})
\end{split} \end{equation*}


\subsection{Continuous Time} In continuos time, state transistions happen
independtant of clock ticks. We can extend the discrete time theory we have
developed by considering the times $t_0$, $t_1$, $t_2$, ... , $t_n$. These are the
times when state transistions occur. \\[0.5cm]

Note that, if we can accept accuracy within a time interval, then we can just
use discrete time. Arrival and departure models generally use continuos time.
This distribution is heavily connected with the exponential function. \\[0.5cm]

\paragraph{Assumption: Continuos Time Markov Chain}
Let $X(t)$ be a random variable, where the distirbution of the random variable
depends on the real parameter t.
For instance, at $t=1.0$ the distribution of X
might be exponential, but at $t=\pi$ the distribution might be normal.
Given the above notation for state transistion, we have the following expression
for the probability of the $(n+1)^{th}$ state transistion.
\begin{equation*} \begin{split}
	P(X(t_{n+1}) = j | X(t_{n}) = i_n, ... ,  X(t_{0}) = i_0)
\end{split} \end{equation*}
Applying the Markov assumption that the next state transistion only depends on
the previous state gives us:
\begin{equation*} \begin{split}
	P(X(t_{n+1}) = j | X(t_{n}) = i_n)
\end{split} \end{equation*}

All the definitions we created in discrete time for recurrent, irreducible, etc
apply to continuos time as well by considering only the times when state
transistions occur.


\paragraph{Transistion Probability} Let the transistion probability be the
probability of going from one state i at time v, to another state at time t.
\begin{equation*} \begin{split}
	P_{ij}(v,t) = P(X(t)=j | X(v) = i)
\end{split} \end{equation*}
There is a special for the above formula:
\begin{equation*} 
	P_{ij}(t,t) =
	\begin{cases}
		1 \mbox{ if } i = j\\
		0 \mbox{ if } i \ne j
	\end{cases}
\end{equation*}


\paragraph{Apply Chapman-Kolmogorov Equation} We can apply the
Chapman-Kolmogorov Equation to the transistion probability. The probability of going
\begin{enumerate}
	\item from state i at time v
	\item to state j at time v
\end{enumerate}
is the same as going
\begin{enumerate}
	\item from state i at time v
	\item to state k at some intermediate time u (summed over all possible k's)
	\item to state j at time t 
\end{enumerate}
gives us the following equation:
\begin{equation*} \begin{split}
	P_{ij}(v,t) = \sum_{k=1}^\infty P_{ik}(v,u)P_{kj}(u,t)
\end{split} \end{equation*}
 
\paragraph{Partial of $P_{ij}(v,t)$} \label{pij-partial-derivation-context}
\begin{equation*} \begin{split}
	\frac{\partial P_{ij}(v,t)}{\partial t}
	= \lim_{\triangle t \to 0}
	\sum_{k \ne j} P_{ik}(v,t) \frac{P_{kj}(t,t+\triangle t)}{\triangle t}
		+ P_{ij}(v,t)\frac{P_{jj}(t,t+\triangle t) -1} {\triangle t}
\end{split} \end{equation*}
For a derivation of this, see (\ref{pij-partial-derivation}).

\paragraph{Rate Definitions for Simplificaiton} We define $q_{ij}$ in order to
simplify the partial expression above. Additionally, we define $q_{ij}$ in such
a way that $\sum_j q_{ij}(t) = 0  \forall i,t$
\begin{equation*} \begin{split}
	q_{kj}(t) = \lim_{\triangle t \to 0} \frac{P_{kj}(t,t+\triangle t)}{\triangle
	t} \mbox{ } k \ne j
\end{split} \end{equation*}
\begin{equation*} \begin{split}
	q_{jj}(t) = \lim_{\triangle t \to 0} \frac{P_{jj}(t,t+\triangle t)-1}{\triangle t}
\end{split} \end{equation*}
This gives us the intended simplication of the partial:
\begin{equation*} \begin{split}
	\frac{\partial P_{ij}(v,t)}{\partial t}
	= \lim_{\triangle t \to 0}
	\sum_{k \ne j} P_{ik}(v,t) q_{kj}(t)
	+ P_{ij}(v,t)q_{jj}(t)
\end{split} \end{equation*}
These $q_{ij}$'s become useful later on as they become the arrival and departure
rates in a birth-death process.


\paragraph{State Probabilitiy} \label{piej-derivative-derivation-context}
Let $\pi_j(t)$ be the probability of being in
state j, at time t.
\begin{equation*} \begin{split}
	\pi_j(t) = P(X(t) = j)
\end{split} \end{equation*}
By applying total probability, over some earlier time v we get:
\begin{equation*} \begin{split}
	\pi_j(t) = \sum_{i} P_{ij}(v,t) \pi_i(v) \mbox{ (which is identical to the discrete case)}
\end{split} \end{equation*}
Taking the derivative we arrive at (see derivation (\ref{piej-derivative-derivation})):
\begin{equation*} \begin{split}
	\frac{d \pi_j(t) }{dt} = \sum_{k\ ne j} \pi_k(t)q_{kj}(t) + \pi_j(t)q_{jj}(t)
\end{split} \end{equation*}
Applying the assumption that the transistion probabilities are homogeneous
(indepenant over time), we have that $q_{ij}(t) = q_{ij}$
\begin{equation*} \begin{split}
	\frac{d \pi_j(t) }{dt} = \sum_{k \ne j} \pi_k(t)q_{kj} + \pi_j(t)q_jj
\end{split} \end{equation*}

\paragraph{Steady State Results}

\paragraph{Residence Time in State}

\begin{equation*} \begin{split}
\end{split} \end{equation*}





